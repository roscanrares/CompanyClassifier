{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unidecode\n",
    "import ast\n",
    "import nltk\n",
    "import spacy\n",
    "from sklearn.cluster import DBSCAN\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import time\n",
    "import unicodedata"
   ],
   "id": "6497fda42e411688",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "nltk.data.path.append(os.path.expanduser('~/nltk_data'))\n",
    "nltk.data.find('tokenizers/punkt')\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "df = pd.read_csv(\"ml_insurance_challenge.csv\", encoding=\"utf-8\")"
   ],
   "id": "595a987427a6245",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "required_columns = [\"description\", \"business_tags\", \"sector\", \"category\", \"niche\"]\n",
    "\n",
    "df[required_columns] = df[required_columns].applymap(\n",
    "    lambda x: 'missing_value' if pd.isna(x) or str(x).strip() == '' else x\n",
    ")\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "processed_texts = []\n",
    "for i in tqdm(range(len(df)), desc=\"Preprocesare text pentru BERT\"):\n",
    "    text = str(df.loc[i, \"description\"]).strip()\n",
    "\n",
    "    text = unidecode.unidecode(text)\n",
    "    text = re.sub(r\"http\\S+|www\\S+|@\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^\\w\\s.,!?]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s.,!?]\", \"\", text)\n",
    "\n",
    "    processed_texts.append(text)"
   ],
   "id": "e79f9d2638a7b76a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "clustering_params = {\n",
    "    \"description\": {\"eps\": 0.4, \"min_samples\": 5},\n",
    "    \"business_tags\": {\"eps\": 0.5, \"min_samples\": 3},\n",
    "    \"sector\": {\"eps\": 0.7, \"min_samples\": 2},\n",
    "    \"category\": {\"eps\": 0.7, \"min_samples\": 2},\n",
    "    \"niche\": {\"eps\": 0.7, \"min_samples\": 2},\n",
    "}"
   ],
   "id": "ec9555c8fd349f64",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for column in required_columns:\n",
    "    print(f\"=== Procesare coloana: {column} ===\")\n",
    "\n",
    "    all_words = set()\n",
    "\n",
    "    for val in tqdm(df[column].fillna(\"\").astype(str), desc=f\"Extragere cuvinte ({column})\", unit=\" randuri\"):\n",
    "        if column == \"business_tags\":\n",
    "            try:\n",
    "                tags_list = ast.literal_eval(val)\n",
    "                if isinstance(tags_list, list):\n",
    "                    for t in tags_list:\n",
    "                        for w in t.split():\n",
    "                            all_words.add(w)\n",
    "            except:\n",
    "                for w in val.split():\n",
    "                    all_words.add(w)\n",
    "        else:\n",
    "            for w in val.split():\n",
    "                all_words.add(w)\n",
    "\n",
    "    if not all_words:\n",
    "        print(f\"Coloana goala: {column}, se sare peste.\")\n",
    "        continue\n",
    "\n",
    "    words_list = list(all_words)\n",
    "    embeddings_list = []\n",
    "\n",
    "    print(f\"Generare embeddings pentru {len(words_list)} cuvinte...\")\n",
    "    for w in tqdm(words_list, desc=\"Creare embeddings\", unit=\" cuvinte\"):\n",
    "        token_vec = nlp(w).vector\n",
    "        embeddings_list.append(token_vec)\n",
    "\n",
    "    embeddings_list = np.array(embeddings_list)\n",
    "\n",
    "    eps_val = clustering_params[column][\"eps\"]\n",
    "    min_samples_val = clustering_params[column][\"min_samples\"]\n",
    "\n",
    "    print(\"Aplic DBSCAN pentru clustering...\")\n",
    "    db = DBSCAN(eps=eps_val, min_samples=min_samples_val, metric=\"cosine\")\n",
    "    labels = db.fit_predict(embeddings_list)\n",
    "\n",
    "    noise_indices = [i for i, lab in enumerate(labels) if lab == -1]\n",
    "    noise_words = {words_list[i] for i in noise_indices}\n",
    "\n",
    "    print(f\"Total cuvinte: {len(words_list)}, Noise detectat: {len(noise_words)}\")\n",
    "\n",
    "    print(f\"Cura»õare {column}...\")\n",
    "\n",
    "    if column == \"description\":\n",
    "        for i in tqdm(range(len(df)), desc=\"Curatare description\", unit=\" randuri\"):\n",
    "            text = str(df.at[i, column])\n",
    "            tokens = text.split()\n",
    "            filtered = [tok for tok in tokens if tok not in noise_words]\n",
    "            # df.at[i, column] = \" \".join(filtered)\n",
    "\n",
    "    elif column == \"business_tags\":\n",
    "        for i in tqdm(range(len(df)), desc=\"Curatare business_tags\", unit=\" randuri\"):\n",
    "            val = str(df.at[i, column])\n",
    "            try:\n",
    "                tags_list = ast.literal_eval(val)\n",
    "                if isinstance(tags_list, list):\n",
    "                    new_tags = []\n",
    "                    for tg in tags_list:\n",
    "                        tag_words = tg.split()\n",
    "                        if any(w in noise_words for w in tag_words):\n",
    "                            continue\n",
    "                        new_tags.append(tg)\n",
    "                    df.at[i, column] = new_tags\n",
    "                else:\n",
    "                    df.at[i, column] = []\n",
    "            except:\n",
    "                df.at[i, column] = []\n",
    "\n",
    "    else:\n",
    "        for i in tqdm(range(len(df)), desc=f\"Curatare {column}\", unit=\" randuri\"):\n",
    "            val = str(df.at[i, column])\n",
    "            tokens = val.split()\n",
    "            if any(t in noise_words for t in tokens):\n",
    "                df.at[i, column] = f\"missing_{column}\""
   ],
   "id": "35152c458826d66e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.to_csv(\"cox.csv\", index=False)",
   "id": "4ffc8839efb22511",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
